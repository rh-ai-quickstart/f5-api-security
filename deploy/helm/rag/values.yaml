replicaCount: 1

image:
  repository: quay.io/rh-ai-quickstart/f5-security-ui
  pullPolicy: Always
  # tag: 0.2.18

service:
  type: ClusterIP
  port: 8501

serviceAccount:
  create: false

livenessProbe:
  httpGet:
    path: /
    port: http

readinessProbe:
  httpGet:
    path: /
    port: http

env:
  - name: LLAMA_STACK_ENDPOINT
    value: 'http://llamastack:8321'
  - name: PGVECTOR_HOST
    value: 'pgvector'
  - name: PGVECTOR_PORT
    value: '5432'
  - name: PGVECTOR_USER
    value: 'postgres'
  - name: PGVECTOR_PASSWORD
    value: 'rag_password'
  - name: PGVECTOR_DB
    value: 'rag_blueprint'

volumes:
  - emptyDir: {}
    name: dot-streamlit

volumeMounts:
  - mountPath: /.streamlit
    name: dot-streamlit

# Common model values for llm-service and llama-stack
# See format in https://github.com/rh-ai-quickstart/ai-architecture-charts/blob/main/llm-service/helm/values.yaml
# For e.g., to add a new model add the following block and it will append to the list of models defined in the llm-service
# 
# Device Support:
# - Use DEVICE=cpu for CPU-only deployment
# - Use DEVICE=gpu for NVIDIA GPU deployment (default)
# - Use DEVICE=hpu for Intel Gaudi HPU deployment (requires Intel Gaudi drivers and setup)

# global:
#   models:
#     llama-4-scout-17b:
#       url: "https://llama-4-scout:443/v1"
#       apiToken: "Paste-your-token-here"
#       id: "llama-4-scout-17b"
#       enabled: true
#     llama-3-2-3b-instruct:
#       id: meta-llama/Llama-3.2-3B-Instruct
#       enabled: true
#       resources:
#         limits:
#           nvidia.com/gpu: "1"
#       tolerations:
#       - key: "nvidia.com/gpu"
#         operator: Exists
#         effect: NoSchedule
#       args:
#       - --enable-auto-tool-choice
#       - --chat-template
#       - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
#       - --tool-call-parser
#       - llama3_json
#       - --max-model-len
#       - "30444"
#     # Example HPU (Intel Gaudi) configuration:
#     llama-3-2-3b-instruct:
#       id: meta-llama/Llama-3.2-3B-Instruct
#       enabled: true
#       device: "hpu"
#       accelerators: "1" # Can be omitted, defaults to 1
#       args:
#       - --max-model-len
#       - "14336"
#       - --max-num-seqs
#       - "32"
#     llama-guard-3-8b:
#       id: meta-llama/Llama-Guard-3-8B
#       enabled: true
#       registerShield: true
#       device: "hpu"    
#       args:
#       - --max-model-len
#       - "14336"
#       - --max-num-seqs
#       - "32"
#     granite-vision-3-2-2b:
#       id: ibm-granite/granite-vision-3.2-2b
#       enabled: true
#       device: "gpu"  # Options: "cpu", "gpu", "hpu"
#       accelerators: "1"
#       args:
#       - --tensor-parallel-size
#       - "1"
#       - --max-model-len
#       - "6144"
#       - --enable-auto-tool-choice
#       - --tool-call-parser
#       - granite
#     qwen25-vl-7b-instruct-fp8-dynamic:
#       id: RedHatAI/Qwen2.5-VL-7B-Instruct-FP8-Dynamic
#       enabled: true      
#       resources:
#         limits:
#           nvidia.com/gpu: "1"
#       tolerations:
#       - key: "nvidia.com/gpu"
#         operator: Exists
#         effect: NoSchedule
#       args:
#       - --distributed-executor-backend=mp
#       - --dtype=auto
#       - --max-model-len=8000

global:
  models: {}

# Hugging Face Token for model downloads
llm-service:
  secret:
    hf_token: ""
    enabled: true

pgvector:
  secret:
    user: postgres
    password: rag_password
    dbname: rag_blueprint
    host: pgvector
    port: "5432"


llama-stack:
  image:
    tag: 0.2.23  # Match frontend client version (was 0.2.22)
  secrets:
    TAVILY_SEARCH_API_KEY: "Paste-your-key-here"


# Suggested Questions Configuration (Optional)
# These questions appear in the chat UI when users select a database
# The key should match the vector_store_name (identifier) of the database
# Example:
# suggestedQuestions:
#   my-api-docs:
#     - "What are the API authentication methods?"
#     - "How do I rate limit API calls?"
#     - "What security features are available?"


